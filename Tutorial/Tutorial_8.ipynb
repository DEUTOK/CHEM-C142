{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4d2a34a",
   "metadata": {},
   "source": [
    "# Tutorial 8 - Recurrent Neural Network\n",
    "\n",
    "## Outline\n",
    "+ Announcement\n",
    "+ Suggestions about using activation function on the final output layer\n",
    "+ Recurrent Neural Network & LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7dd5cb",
   "metadata": {},
   "source": [
    "## Announcement\n",
    "\n",
    "+ **NO** tutorial session next week!\n",
    "+ Eric's office hours:\n",
    "    - March 14th (Thursday) 6:30-7:30 pm\n",
    "    - March 19th (Tuesday) 6:30-7:30 pm\n",
    "    - Zoom: https://berkeley.zoom.us/j/7510266955\n",
    "+ HW8 is released today and due next Thursday (which is the ugrad mid-term date, please start early on this HW)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13484490",
   "metadata": {},
   "source": [
    "## Suggestions about using activation functions\n",
    "\n",
    "In most cases, the output layer is not being activated because the activation function will shrink the output range, which disable the model fit to data out of the range. For example, tanh will give output between -1 and 1, so if the targets range from $(-2, 2)$, the model will fail to learn. \n",
    "\n",
    "But if the targets are probabilities, it's better to use Sigmoid or Softmax, which will enforce an output value in $(0,1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b5c987",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network\n",
    "\n",
    "RNN is a series of architecutres that is designed for sequential data, such as audio and text.\n",
    "\n",
    "### Vanilla RNN\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Recurrent_neural_network_unfold.svg/2880px-Recurrent_neural_network_unfold.svg.png\" width=\"800\" />\n",
    "\n",
    "+ Inputs: \n",
    "    \n",
    "    - $\\boldsymbol{X}(X_1,X_2,\\cdots,X_t)$\n",
    "    \n",
    "    - $h_0$\n",
    "    \n",
    "+ Feed forward:\n",
    "    \n",
    "    $$h_t = \\sigma(x_t W_{ih}^T + b_{ih} + h_{t-1}W_{hh}^T + b_{hh})$$\n",
    "    \n",
    "    $$y_t = \\sigma(h_t W_{oh}^T + b_{oh}) $$\n",
    "   \n",
    "+ PyTorch: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html#torch.nn.RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1410f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog(\"rdApp.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a28883a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.7851,  0.3899,  0.0067],\n",
      "         [-0.7241,  0.1772,  0.4840]]], grad_fn=<TransposeBackward1>)\n",
      "tensor([[[-0.7241,  0.1772,  0.4840]]], grad_fn=<StackBackward0>) torch.Size([1, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "# nn.RNN(input_dim, hidden_dim, num_layers)\n",
    "rnn = nn.RNN(5,3,1, batch_first = True)\n",
    "\n",
    "# input shape: (n_batch, n_seq, input_dim)\n",
    "inputs = torch.rand(1, 2, 5)\n",
    "\n",
    "# h0 shape: (n_layers, n_batch, hiden_dim)\n",
    "h0 = torch.rand(1, 1, 3)\n",
    "\n",
    "# output(h1,...,ht), ht\n",
    "output, ht = rnn(inputs, h0)\n",
    "\n",
    "print(output)\n",
    "print(ht , ht.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b61cba69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.7297,  0.4705,  0.1410],\n",
       "          [-0.7179,  0.1370,  0.4425]]], grad_fn=<TransposeBackward1>),\n",
       " tensor([[[-0.7179,  0.1370,  0.4425]]], grad_fn=<StackBackward0>))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# without explicitly setting h0\n",
    "output, ht = rnn(inputs)\n",
    "output, ht"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167491a4",
   "metadata": {},
   "source": [
    "### LSTM: Long-short Term Memory\n",
    "\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mb_L_slY9rjMr8-IADHvwg.png\" width=\"600\" />\n",
    "\n",
    "+ Inputs: \n",
    "    \n",
    "    - $\\boldsymbol{X}(X_1,X_2,\\cdots,X_t)$\n",
    "    \n",
    "    - $h_0$\n",
    "    \n",
    "    - $c_0$\n",
    "    \n",
    "+ Feed forward:\n",
    "    \n",
    "    $$\\begin{array}{ll} \\\\\n",
    "            i_t = \\mathrm{sigmoid}(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\\\\n",
    "            f_t = \\mathrm{sigmoid}(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\\\\n",
    "            g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\\\\n",
    "            o_t = \\mathrm{sigmoid}(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\\\\n",
    "            c_t = f_t \\odot c_{t-1} + i_t \\odot g_t \\\\\n",
    "            h_t = o_t \\odot \\tanh(c_t) \\\\\n",
    "        \\end{array}$$\n",
    "\n",
    "+ PyTorch: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67f90318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1444,  0.1238,  0.2167],\n",
      "         [ 0.1561, -0.0755,  0.3056]]], grad_fn=<TransposeBackward0>)\n",
      "tensor([[[ 0.1561, -0.0755,  0.3056]]], grad_fn=<StackBackward0>)\n",
      "tensor([[[ 0.6738, -0.1312,  0.6685]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "lstm = nn.LSTM(5, 3, 1, batch_first = True)\n",
    "\n",
    "# input shape: (n_batch, n_seq, input_dim)\n",
    "inputs = torch.rand(1, 2, 5)\n",
    "\n",
    "# hidden shape: (n_layers, n_batch, hiden_dim)\n",
    "h0 = torch.rand(1, 1, 3)\n",
    "c0 = torch.rand(1, 1, 3)\n",
    "\n",
    "# output: h1, ... ht\n",
    "# ht, ct\n",
    "output, (ht, ct) = lstm(inputs, (h0, c0))\n",
    "\n",
    "print(output)\n",
    "print(ht)\n",
    "print(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd58e0e6",
   "metadata": {},
   "source": [
    "### Generate SMILES strings using RNN\n",
    "\n",
    "Data pre-processing:\n",
    "\n",
    "+ Add starting/ending tokens\n",
    "\n",
    "    - `SOS`: Start Of Sequence\n",
    "\n",
    "    - `EOS`: End Of Sequence\n",
    "\n",
    "+ One-hot Encoding\n",
    "\n",
    "+ Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8381aeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', 'N', 'O', 'CC', 'CN']\n"
     ]
    }
   ],
   "source": [
    "def load_smiles(path):\n",
    "    with open(path) as f:\n",
    "        smiles = f.read().split('\\n')\n",
    "    return smiles\n",
    "\n",
    "smiles = load_smiles(\"/Users/mac_1/Desktop/CHEM C142/ani_smiles_clean.txt\")\n",
    "print(smiles[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8e0717",
   "metadata": {},
   "source": [
    "Padding: `\"C=CC#N\" -> ['SOS, 'C', '=', 'C', 'C', '#', 'N', 'EOS']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56a1117e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['SOS', 'C', 'EOS'], ['SOS', 'N', 'EOS'], ['SOS', 'O', 'EOS'], ['SOS', 'C', 'C', 'EOS'], ['SOS', 'C', 'N', 'EOS']]\n"
     ]
    }
   ],
   "source": [
    "def pad_start_end_token(smiles):\n",
    "    padded = []\n",
    "    for smi in smiles:\n",
    "        padded.append([\"SOS\"] + list(smi) + [\"EOS\"])\n",
    "    return padded\n",
    "\n",
    "\n",
    "padded_smiles = pad_start_end_token(smiles)\n",
    "print(padded_smiles[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641ccdc6",
   "metadata": {},
   "source": [
    "Vocabulary: unique tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7cb38c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['#', '(', ')', '1', '2', '=', 'C', 'EOS', 'H', 'N', 'O', 'SOS',\n",
       "       '[', ']', 'c', 'n', 'o'], dtype='<U3')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = np.unique(np.concatenate(padded_smiles))\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b4830fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1771, 17, 17])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SmilesDataset(Dataset):\n",
    "    def __init__(self, smiles, vocab):\n",
    "        \n",
    "        self.vocab = np.array(vocab, dtype=str).reshape(-1, 1)\n",
    "        \n",
    "        # One-hot encoding\n",
    "        self.encoder = OneHotEncoder()\n",
    "        self.encoder.fit(self.vocab)\n",
    "        \n",
    "        self.data = [\n",
    "            torch.tensor(\n",
    "                self.encoder.transform(np.array(s).reshape(-1,1)).toarray(), # transform data\n",
    "                dtype=torch.float\n",
    "            ) for s in smiles\n",
    "        ]\n",
    "        \n",
    "        # Padding: nn.utils.rnn.pad_sequence\n",
    "        # shape: (n_samples, n_sequence, n_tokens)\n",
    "        self.data = nn.utils.rnn.pad_sequence(self.data, batch_first = True)\n",
    "        \n",
    "        self.X = self.data[:, :-1, :]\n",
    "        self.y = self.data[:, 1:, :]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(self.data.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "    \n",
    "data = SmilesDataset(padded_smiles, vocab)\n",
    "input_size = data.vocab.shape[0] # should be 17\n",
    "data.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9360f01c",
   "metadata": {},
   "source": [
    "#### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "819def7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, input_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self, x, h=None):\n",
    "        # rnn\n",
    "        output, h = self.rnn(x,h)\n",
    "        # fc\n",
    "        out = self.fc(out)\n",
    "        # softmax\n",
    "        out = self.softmax(out)\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdb3282",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0523770",
   "metadata": {},
   "source": [
    "Training: try to predict the output tokens given inputs. \n",
    "\n",
    "For example, a valid SMILES is `['SOS', 'C', 'N', 'EOS']`. Give model `['SOS', 'C', 'N']`, and try to let the model output `['C', 'N', 'EOS']`. In this way, the model can learn some information about probability distribution of the output tokens given inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39bc3b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, opt_method, learning_rate, batch_size, epoch, l2):\n",
    "        self.model = model\n",
    "        if opt_method == \"sgdm\":\n",
    "            self.optimizer = torch.optim.SGD(model.parameters(), learning_rate, momentum=0.9)\n",
    "        elif opt_method == \"adam\":\n",
    "            self.optimizer = torch.optim.Adam(model.parameters(), learning_rate, weight_decay=l2)\n",
    "        else:\n",
    "            raise NotImplementedError(\"This optimization is not supported\")\n",
    "        \n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    \n",
    "    def train(self, train_data, draw_curve=True):\n",
    "        self.encoder = train_data.encoder\n",
    "        \n",
    "        train_loader = DataLoader(train_data, batch_size=self.batch_size, shuffle=True)\n",
    "        train_loss_list, train_acc_list = [], []\n",
    "        \n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "        for n in tqdm(range(self.epoch), leave=False):\n",
    "            self.model.train()\n",
    "            epoch_loss, epoch_acc = 0.0, 0.0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                batch_importance = y_batch.shape[0] / len(train_data)\n",
    "                \n",
    "                # batch outputs\n",
    "                y_pred, _ = self.model(X_batch)\n",
    "                \n",
    "                # loss func\n",
    "                batch_loss = loss_func(y_pred, y_batch)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # record accuracy\n",
    "                batch_acc = torch.sum(torch.argmax(y_pred, axis = -1) == torch.argmax(y_batch,axis =-1))/y_batch.shape[0] / y_batch.shape[1]\n",
    "                \n",
    "                epoch_acc += batch_acc.detach().cpu().item() * batch_importance\n",
    "                epoch_loss += batch_loss.detach().cpu().item() * batch_importance\n",
    "            \n",
    "            train_acc_list.append(epoch_acc)\n",
    "            train_loss_list.append(epoch_loss)\n",
    "            \n",
    "        if draw_curve:\n",
    "            x_axis = np.arange(self.epoch)\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "            axes[0].plot(x_axis, train_loss_list, label=\"Train\")\n",
    "            axes[0].set_title(\"Loss\")\n",
    "            axes[0].legend()\n",
    "            axes[1].plot(x_axis, train_acc_list, label='Train')\n",
    "            axes[1].set_title(\"Accuracy\")\n",
    "            axes[1].legend()\n",
    "    \n",
    "    def sample(self, num_seq=10):\n",
    "        self.model.eval()\n",
    "        seqs = []\n",
    "        with torch.no_grad():\n",
    "            for _ in tqdm(range(num_seq), leave=False):\n",
    "                chars = ['SOS']\n",
    "                hidden = self.model.init_hidden(1)\n",
    "                while chars[-1] != 'EOS':\n",
    "                    input_encoding = self.encoder.transform(np.array([chars[-1]]).reshape(-1, 1)).toarray()\n",
    "                    input_encoding = torch.tensor(input_encoding, dtype=torch.float).reshape(1, 1, -1)\n",
    "                    out, hidden = self.model(input_encoding, hidden)\n",
    "\n",
    "                    prob = out.detach().numpy().flatten()\n",
    "                    prob /= np.sum(prob)\n",
    "\n",
    "                    index = np.random.choice(self.model.input_size, p=prob)\n",
    "                    out_encoding = np.zeros((1, self.model.input_size))\n",
    "                    out_encoding[0, index] = 1.0\n",
    "                    char = data.encoder.inverse_transform(out_encoding).flatten().tolist()[0]\n",
    "                    chars.append(char)\n",
    "                seqs.append(''.join(chars[1:-1]))\n",
    "        return seqs\n",
    "\n",
    "\n",
    "def validate(seq):\n",
    "    num = len(seq)\n",
    "    unique = set(seq)\n",
    "    valid = []\n",
    "    for s in unique:\n",
    "        mol = Chem.MolFromSmiles(s)\n",
    "        if mol is not None:\n",
    "            valid.append(s)\n",
    "            \n",
    "    print(f\"Number of unique SMILES: {len(unique)}\")\n",
    "    print(f\"Number of valid & unique SMILES: {len(valid)}\")\n",
    "    return valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "50f1d144",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'out' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/d9/6wjn536x21n4fgb_hwj80bx80000gp/T/ipykernel_91654/3010874637.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVanillaRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mseqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/d9/6wjn536x21n4fgb_hwj80bx80000gp/T/ipykernel_91654/2954787753.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_data, draw_curve)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0;31m# batch outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                 \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;31m# loss func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/d9/6wjn536x21n4fgb_hwj80bx80000gp/T/ipykernel_91654/3593270341.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, h)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# fc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;31m# softmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'out' referenced before assignment"
     ]
    }
   ],
   "source": [
    "model = VanillaRNN(input_size, 32, 1)\n",
    "trainer = Trainer(model, \"adam\", 1e-3, 128, 500, 1e-5)\n",
    "trainer.train(data)\n",
    "seqs = trainer.sample(1000)\n",
    "validate(seqs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
