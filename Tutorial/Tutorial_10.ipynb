{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27df8ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ff7c44",
   "metadata": {},
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "378af35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, in_channels=1, z_dim=32):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 4, kernel_size = 4, padding=1, stride=2), # conv1, input_channel -> 4\n",
    "            nn.ReLU(), # relu\n",
    "            nn.Conv2d(4, 8, kernel_size=4,padding=1, stride=2), # conv2, channel 4 -> 8\n",
    "            nn.ReLU(), # relu\n",
    "            nn.Conv2d(8, 16, kernel_size=4,padding=1, stride=2), # conv3, channel 8 -> 16\n",
    "            nn.ReLU(), # relu\n",
    "            nn.Conv2d(16, 32, kernel_size=4,padding=1, stride=2), # conv4, channel 16 -> 32\n",
    "            nn.ReLU(), # relu\n",
    "            nn.Flatten(), # flatten\n",
    "        )\n",
    "        \n",
    "        # manually calculate the dimension after all convolutions\n",
    "        dim_after_conv = 2\n",
    "        hidden_dim = 32 * dim_after_conv * dim_after_conv\n",
    "        \n",
    "        self.readout_mu = nn.Linear(hidden_dim, z_dim)\n",
    "        self.readout_sigma = nn.Linear(hidden_dim, z_dim)\n",
    "        \n",
    "        # You can use nn.ConvTranspose2d to decode\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(z_dim, hidden_dim),\n",
    "            nn.Unflatten(1, (32, dim_after_conv, dim_after_conv)),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1), # transpose-conv, channel 32 -> 16\n",
    "            nn.ReLU(), # relu\n",
    "            nn.ConvTranspose2d(16, 8, kernel_size=4, stride=2, padding=1), # transpose-conv, channel 16 -> 8\n",
    "            nn.ReLU(), # relu\n",
    "            nn.ConvTranspose2d(8, 4, kernel_size=4, stride=2, padding=1), # transpose-conv, channel 8 -> 4\n",
    "            nn.ReLU(), # relu\n",
    "            nn.ConvTranspose2d(4, 1, kernel_size=4, stride=2, padding=1), # transpose-conv, channel 4 -> input_channel, which is 1\n",
    "            nn.Sigmoid(), # use a sigmoid activation to squeeze the outputs between 0 and 1\n",
    "        )\n",
    "    \n",
    "    def reparameterize(self, mu, sigma):\n",
    "        \"\"\"\n",
    "        Reparameterize, i.e. generate a z ~ N(\\mu, \\sigma)\n",
    "        \"\"\"\n",
    "        # generate epsilon ~ N(0, I)\n",
    "        # hint: use torch.randn or torch.randn_like\n",
    "        epsilon = torch.rand_like(sigma)\n",
    "        # z = \\mu + \\sigma * \\epsilon\n",
    "        z = mu + sigma + epsilon\n",
    "        return z\n",
    "\n",
    "    def encode(self, x):\n",
    "        # call the encoder to map input to a hidden state vector\n",
    "        h = self.encoder(x)\n",
    "        # use the \"readout\" layer to get \\mu and \\sigma\n",
    "        mu = self.readout_mu(h)\n",
    "        sigma = self.readout_sigma(h)\n",
    "        return mu, sigma\n",
    "\n",
    "    def decode(self, z):\n",
    "        # call the decoder to map z back to x\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, sigma = self.encode(x)\n",
    "        z = self.reparameterize(mu, sigma)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7201ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0.5114, 0.5127, 0.5113,  ..., 0.5140, 0.5116, 0.5173],\n",
       "           [0.5197, 0.5044, 0.5273,  ..., 0.5048, 0.5266, 0.5108],\n",
       "           [0.5127, 0.5244, 0.5173,  ..., 0.5254, 0.5194, 0.5215],\n",
       "           ...,\n",
       "           [0.5182, 0.5051, 0.5268,  ..., 0.5060, 0.5257, 0.5109],\n",
       "           [0.5133, 0.5246, 0.5175,  ..., 0.5249, 0.5179, 0.5234],\n",
       "           [0.5194, 0.5157, 0.5207,  ..., 0.5151, 0.5206, 0.5174]]],\n",
       " \n",
       " \n",
       "         [[[0.5114, 0.5127, 0.5113,  ..., 0.5139, 0.5114, 0.5174],\n",
       "           [0.5198, 0.5045, 0.5272,  ..., 0.5047, 0.5272, 0.5108],\n",
       "           [0.5128, 0.5245, 0.5174,  ..., 0.5255, 0.5194, 0.5213],\n",
       "           ...,\n",
       "           [0.5182, 0.5050, 0.5268,  ..., 0.5061, 0.5252, 0.5110],\n",
       "           [0.5132, 0.5243, 0.5177,  ..., 0.5252, 0.5180, 0.5235],\n",
       "           [0.5193, 0.5157, 0.5206,  ..., 0.5150, 0.5206, 0.5174]]],\n",
       " \n",
       " \n",
       "         [[[0.5114, 0.5126, 0.5113,  ..., 0.5138, 0.5114, 0.5173],\n",
       "           [0.5196, 0.5043, 0.5273,  ..., 0.5052, 0.5266, 0.5109],\n",
       "           [0.5128, 0.5243, 0.5174,  ..., 0.5249, 0.5191, 0.5215],\n",
       "           ...,\n",
       "           [0.5182, 0.5051, 0.5269,  ..., 0.5060, 0.5256, 0.5109],\n",
       "           [0.5133, 0.5246, 0.5183,  ..., 0.5246, 0.5180, 0.5233],\n",
       "           [0.5193, 0.5157, 0.5204,  ..., 0.5151, 0.5205, 0.5174]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0.5114, 0.5127, 0.5114,  ..., 0.5141, 0.5117, 0.5173],\n",
       "           [0.5198, 0.5047, 0.5271,  ..., 0.5047, 0.5268, 0.5109],\n",
       "           [0.5128, 0.5249, 0.5176,  ..., 0.5260, 0.5195, 0.5215],\n",
       "           ...,\n",
       "           [0.5183, 0.5051, 0.5269,  ..., 0.5061, 0.5256, 0.5109],\n",
       "           [0.5133, 0.5248, 0.5180,  ..., 0.5252, 0.5178, 0.5233],\n",
       "           [0.5194, 0.5157, 0.5206,  ..., 0.5151, 0.5207, 0.5174]]],\n",
       " \n",
       " \n",
       "         [[[0.5115, 0.5126, 0.5113,  ..., 0.5139, 0.5115, 0.5174],\n",
       "           [0.5197, 0.5044, 0.5272,  ..., 0.5051, 0.5266, 0.5108],\n",
       "           [0.5127, 0.5248, 0.5177,  ..., 0.5248, 0.5191, 0.5214],\n",
       "           ...,\n",
       "           [0.5183, 0.5049, 0.5269,  ..., 0.5060, 0.5254, 0.5109],\n",
       "           [0.5133, 0.5237, 0.5174,  ..., 0.5246, 0.5180, 0.5234],\n",
       "           [0.5194, 0.5156, 0.5209,  ..., 0.5151, 0.5205, 0.5174]]],\n",
       " \n",
       " \n",
       "         [[[0.5114, 0.5126, 0.5113,  ..., 0.5137, 0.5113, 0.5174],\n",
       "           [0.5197, 0.5043, 0.5273,  ..., 0.5050, 0.5269, 0.5108],\n",
       "           [0.5127, 0.5245, 0.5175,  ..., 0.5245, 0.5189, 0.5215],\n",
       "           ...,\n",
       "           [0.5182, 0.5052, 0.5269,  ..., 0.5060, 0.5257, 0.5110],\n",
       "           [0.5132, 0.5248, 0.5181,  ..., 0.5249, 0.5179, 0.5235],\n",
       "           [0.5193, 0.5157, 0.5205,  ..., 0.5151, 0.5206, 0.5174]]]],\n",
       "        grad_fn=<SigmoidBackward0>),\n",
       " tensor([[ 0.0213,  0.0258, -0.0737, -0.0515, -0.0785, -0.0472,  0.0272,  0.0549,\n",
       "          -0.0698, -0.0235, -0.0386, -0.0315, -0.0266,  0.0754,  0.0911, -0.0323,\n",
       "          -0.0844, -0.0434, -0.0707,  0.0438,  0.0011,  0.0759,  0.0803, -0.0337,\n",
       "          -0.0481, -0.0456, -0.0046,  0.0686,  0.0547, -0.0681,  0.0432, -0.0435],\n",
       "         [ 0.0156,  0.0217, -0.0782, -0.0550, -0.0776, -0.0502,  0.0276,  0.0522,\n",
       "          -0.0700, -0.0253, -0.0375, -0.0295, -0.0282,  0.0778,  0.0922, -0.0337,\n",
       "          -0.0827, -0.0401, -0.0715,  0.0396,  0.0010,  0.0793,  0.0824, -0.0346,\n",
       "          -0.0504, -0.0496, -0.0044,  0.0711,  0.0555, -0.0690,  0.0421, -0.0438],\n",
       "         [ 0.0149,  0.0278, -0.0774, -0.0510, -0.0791, -0.0473,  0.0290,  0.0542,\n",
       "          -0.0666, -0.0237, -0.0372, -0.0302, -0.0264,  0.0779,  0.0950, -0.0333,\n",
       "          -0.0822, -0.0419, -0.0696,  0.0407,  0.0011,  0.0818,  0.0802, -0.0350,\n",
       "          -0.0514, -0.0505, -0.0076,  0.0695,  0.0549, -0.0689,  0.0420, -0.0421],\n",
       "         [ 0.0188,  0.0245, -0.0789, -0.0556, -0.0747, -0.0504,  0.0279,  0.0539,\n",
       "          -0.0676, -0.0241, -0.0386, -0.0328, -0.0297,  0.0784,  0.0914, -0.0319,\n",
       "          -0.0816, -0.0412, -0.0701,  0.0436, -0.0006,  0.0795,  0.0807, -0.0359,\n",
       "          -0.0498, -0.0488, -0.0039,  0.0684,  0.0566, -0.0706,  0.0450, -0.0487],\n",
       "         [ 0.0182,  0.0235, -0.0782, -0.0567, -0.0792, -0.0508,  0.0279,  0.0555,\n",
       "          -0.0699, -0.0275, -0.0369, -0.0288, -0.0295,  0.0759,  0.0910, -0.0355,\n",
       "          -0.0843, -0.0428, -0.0706,  0.0389, -0.0018,  0.0795,  0.0802, -0.0347,\n",
       "          -0.0499, -0.0498, -0.0021,  0.0707,  0.0526, -0.0706,  0.0432, -0.0418],\n",
       "         [ 0.0151,  0.0251, -0.0793, -0.0540, -0.0796, -0.0507,  0.0314,  0.0490,\n",
       "          -0.0676, -0.0228, -0.0356, -0.0332, -0.0271,  0.0771,  0.0945, -0.0340,\n",
       "          -0.0836, -0.0406, -0.0719,  0.0420,  0.0016,  0.0817,  0.0828, -0.0331,\n",
       "          -0.0515, -0.0480, -0.0076,  0.0695,  0.0562, -0.0691,  0.0454, -0.0436],\n",
       "         [ 0.0185,  0.0273, -0.0784, -0.0559, -0.0768, -0.0487,  0.0309,  0.0537,\n",
       "          -0.0713, -0.0255, -0.0401, -0.0301, -0.0281,  0.0763,  0.0961, -0.0339,\n",
       "          -0.0820, -0.0414, -0.0682,  0.0414, -0.0004,  0.0808,  0.0814, -0.0337,\n",
       "          -0.0475, -0.0479, -0.0092,  0.0691,  0.0554, -0.0712,  0.0426, -0.0437],\n",
       "         [ 0.0180,  0.0270, -0.0776, -0.0540, -0.0770, -0.0482,  0.0316,  0.0535,\n",
       "          -0.0690, -0.0268, -0.0371, -0.0341, -0.0301,  0.0793,  0.0936, -0.0369,\n",
       "          -0.0814, -0.0405, -0.0703,  0.0415,  0.0013,  0.0789,  0.0846, -0.0363,\n",
       "          -0.0515, -0.0485, -0.0073,  0.0691,  0.0533, -0.0688,  0.0427, -0.0429],\n",
       "         [ 0.0183,  0.0241, -0.0756, -0.0572, -0.0760, -0.0520,  0.0298,  0.0538,\n",
       "          -0.0736, -0.0265, -0.0371, -0.0294, -0.0315,  0.0783,  0.0939, -0.0331,\n",
       "          -0.0842, -0.0408, -0.0715,  0.0403, -0.0034,  0.0795,  0.0828, -0.0355,\n",
       "          -0.0493, -0.0474, -0.0050,  0.0728,  0.0557, -0.0717,  0.0438, -0.0429],\n",
       "         [ 0.0174,  0.0272, -0.0810, -0.0568, -0.0746, -0.0502,  0.0290,  0.0533,\n",
       "          -0.0711, -0.0234, -0.0396, -0.0297, -0.0293,  0.0769,  0.0912, -0.0337,\n",
       "          -0.0833, -0.0407, -0.0682,  0.0424, -0.0012,  0.0824,  0.0819, -0.0362,\n",
       "          -0.0509, -0.0476, -0.0052,  0.0720,  0.0561, -0.0685,  0.0459, -0.0473]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-0.0686,  0.0088, -0.0249,  0.0014, -0.0237, -0.0129, -0.0343,  0.0391,\n",
       "           0.0089,  0.0033, -0.0383,  0.0530,  0.0728, -0.0541,  0.0693, -0.0789,\n",
       "           0.0703,  0.0600,  0.0703, -0.0791,  0.0200,  0.0292, -0.0345, -0.0468,\n",
       "           0.0405, -0.0282, -0.0502,  0.0511, -0.0606, -0.0344, -0.0064, -0.0542],\n",
       "         [-0.0689,  0.0081, -0.0263,  0.0031, -0.0241, -0.0149, -0.0334,  0.0392,\n",
       "           0.0103,  0.0079, -0.0382,  0.0513,  0.0744, -0.0550,  0.0703, -0.0781,\n",
       "           0.0696,  0.0623,  0.0721, -0.0843,  0.0216,  0.0275, -0.0326, -0.0452,\n",
       "           0.0398, -0.0295, -0.0505,  0.0480, -0.0620, -0.0362, -0.0048, -0.0553],\n",
       "         [-0.0685,  0.0094, -0.0288,  0.0022, -0.0249, -0.0122, -0.0307,  0.0373,\n",
       "           0.0102,  0.0065, -0.0358,  0.0520,  0.0759, -0.0572,  0.0716, -0.0797,\n",
       "           0.0724,  0.0619,  0.0689, -0.0804,  0.0213,  0.0285, -0.0305, -0.0446,\n",
       "           0.0419, -0.0299, -0.0523,  0.0472, -0.0589, -0.0347, -0.0049, -0.0532],\n",
       "         [-0.0664,  0.0077, -0.0266,  0.0064, -0.0221, -0.0133, -0.0338,  0.0364,\n",
       "           0.0081,  0.0065, -0.0427,  0.0535,  0.0753, -0.0542,  0.0689, -0.0772,\n",
       "           0.0715,  0.0608,  0.0740, -0.0821,  0.0186,  0.0310, -0.0346, -0.0479,\n",
       "           0.0418, -0.0277, -0.0497,  0.0481, -0.0620, -0.0357, -0.0069, -0.0542],\n",
       "         [-0.0687,  0.0077, -0.0273,  0.0012, -0.0266, -0.0134, -0.0316,  0.0376,\n",
       "           0.0146,  0.0121, -0.0377,  0.0536,  0.0766, -0.0579,  0.0691, -0.0761,\n",
       "           0.0675,  0.0621,  0.0746, -0.0816,  0.0203,  0.0263, -0.0329, -0.0445,\n",
       "           0.0421, -0.0296, -0.0483,  0.0488, -0.0630, -0.0354, -0.0029, -0.0551],\n",
       "         [-0.0659,  0.0082, -0.0257,  0.0034, -0.0244, -0.0143, -0.0311,  0.0365,\n",
       "           0.0084,  0.0018, -0.0381,  0.0527,  0.0768, -0.0531,  0.0683, -0.0823,\n",
       "           0.0698,  0.0637,  0.0706, -0.0822,  0.0188,  0.0279, -0.0339, -0.0475,\n",
       "           0.0448, -0.0307, -0.0528,  0.0476, -0.0598, -0.0331, -0.0028, -0.0525],\n",
       "         [-0.0667,  0.0067, -0.0311,  0.0066, -0.0228, -0.0141, -0.0325,  0.0368,\n",
       "           0.0085,  0.0054, -0.0377,  0.0526,  0.0760, -0.0550,  0.0690, -0.0805,\n",
       "           0.0713,  0.0629,  0.0681, -0.0833,  0.0199,  0.0277, -0.0333, -0.0462,\n",
       "           0.0428, -0.0282, -0.0517,  0.0492, -0.0615, -0.0328, -0.0041, -0.0533],\n",
       "         [-0.0699,  0.0108, -0.0278,  0.0013, -0.0239, -0.0153, -0.0336,  0.0368,\n",
       "           0.0093,  0.0054, -0.0378,  0.0528,  0.0757, -0.0538,  0.0725, -0.0798,\n",
       "           0.0713,  0.0605,  0.0705, -0.0816,  0.0197,  0.0270, -0.0369, -0.0462,\n",
       "           0.0405, -0.0284, -0.0527,  0.0494, -0.0607, -0.0345, -0.0058, -0.0555],\n",
       "         [-0.0703,  0.0049, -0.0247,  0.0059, -0.0235, -0.0152, -0.0332,  0.0404,\n",
       "           0.0086,  0.0084, -0.0431,  0.0510,  0.0783, -0.0538,  0.0700, -0.0752,\n",
       "           0.0673,  0.0590,  0.0719, -0.0846,  0.0244,  0.0285, -0.0346, -0.0455,\n",
       "           0.0421, -0.0301, -0.0491,  0.0482, -0.0608, -0.0345, -0.0047, -0.0541],\n",
       "         [-0.0679,  0.0063, -0.0281,  0.0059, -0.0210, -0.0132, -0.0313,  0.0374,\n",
       "           0.0082,  0.0072, -0.0392,  0.0512,  0.0754, -0.0599,  0.0703, -0.0788,\n",
       "           0.0695,  0.0640,  0.0750, -0.0825,  0.0208,  0.0318, -0.0333, -0.0420,\n",
       "           0.0410, -0.0300, -0.0479,  0.0464, -0.0607, -0.0337, -0.0053, -0.0562]],\n",
       "        grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae = VAE()\n",
    "vae(torch.rand(10, 1, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2b61c5",
   "metadata": {},
   "source": [
    "# GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ce714b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.loader import DataLoader as GraphDataLoader\n",
    "from torch_geometric.utils import scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1723d792",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_qm9(path=\"./QM9\"):\n",
    "    def transform(data):\n",
    "        edge_index = torch.tensor(\n",
    "            list(itertools.permutations(range(data.x.shape[0]), 2)), \n",
    "            dtype=torch.long\n",
    "        ).T\n",
    "        edge_feature = 1 / torch.sqrt(\n",
    "            torch.sum(\n",
    "                (data.pos[edge_index[0]] - data.pos[edge_index[1]]) ** 2, \n",
    "                axis=1, keepdim=True\n",
    "            )\n",
    "        )\n",
    "        data.edge_index = edge_index\n",
    "        data.edge_attr = edge_feature\n",
    "        data.y = data.y[:, [-7]]\n",
    "        return data\n",
    "    \n",
    "    qm9 = QM9(path, transform=transform)\n",
    "    return qm9\n",
    "\n",
    "qm9 = load_qm9(\"../../Datasets/QM9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "927515f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic layer, a linear layer with a ReLU activation \n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim), # linear layer\n",
    "            nn.ReLU() # relu\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    \n",
    "class MessagePassingLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A message passing layer that updates nodes/edge features\n",
    "    \"\"\"\n",
    "    def __init__(self, node_hidden_dim, edge_hidden_dim):\n",
    "        super().__init__()\n",
    "        # figure out the input/output dimension\n",
    "        self.edge_net = Layer(2*node_hidden_dim + edge_hidden_dim, edge_hidden_dim)\n",
    "        # figure out the input/output dimension\n",
    "        self.node_net = Layer(node_hidden_dim + edge_hidden_dim, node_hidden_dim)\n",
    "    \n",
    "    def forward(self, node_features, edge_features, edge_index):\n",
    "        \"\"\"\n",
    "        Update node and edge features\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        node_features: torch.Tensor\n",
    "            Node features from the previous layer\n",
    "        edge_features: torch.Tensor\n",
    "            Edge features from the previous layer\n",
    "        edge_index: torch.Tensor\n",
    "            A sparse matrix (n_edge, 2) in which each column denotes node indices forming an edge\n",
    "        \"\"\"\n",
    "        # concatnate previous edge features with node features forming the edge\n",
    "        # hint: use edge_features[edge_index[0(or 1)]] to get node features forming the edge\n",
    "        concate_edge_features = torch.cat([\n",
    "            node_features[edge_index[0]], # features of one node\n",
    "            node_features[edge_index[1]], # features of the other node\n",
    "            edge_features # previous edge features\n",
    "        ], dim=1)\n",
    "        \n",
    "        # pass through the \"edge_net\" to map it back to the original dimension\n",
    "        updated_edge_features = self.edge_net(concate_edge_features)\n",
    "        \n",
    "        \n",
    "        # use scatter to aggrate the edge features to nodes\n",
    "        aggr_edge_features = scatter(updated_edge_features, edge_index[0])\n",
    "        # concatenate it with previous node features\n",
    "        concate_node_features = torch.cat([aggr_edge_features, node_features], dim=1)\n",
    "        # pass through the \"node_net\" to map it back to the original dimension\n",
    "        updated_node_features = self.node_net(concate_node_features)\n",
    "        \n",
    "        return updated_node_features, updated_edge_features\n",
    "\n",
    "        \n",
    "class GraphNet(nn.Module):\n",
    "    def __init__(self, node_input_dim, edge_input_dim, node_hidden_dim, edge_hidden_dim):\n",
    "        super().__init__()\n",
    "        # embed the input node features\n",
    "        self.node_embed = Layer(node_input_dim, node_hidden_dim)\n",
    "        # embed the input edge features\n",
    "        self.edge_embed = Layer(edge_input_dim, edge_hidden_dim)\n",
    "        # use a linear layer as readout to get the \"atomic\" energy contribution\n",
    "        self.readout = nn.Linear(node_input_dim, 1)\n",
    "        # message passing layer\n",
    "        self.message_passing = MessagePassingLayer(node_hidden_dim, edge_hidden_dim) \n",
    "    \n",
    "    def forward(self, node_features, edge_features, edge_index, batch):\n",
    "        \"\"\"\n",
    "        Update node and edge features\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        node_features: torch.Tensor\n",
    "            Node features from the previous layer\n",
    "        edge_features: torch.Tensor\n",
    "            Edge features from the previous layer\n",
    "        edge_index: torch.Tensor\n",
    "            A sparse matrix (n_edges, 2) in which each column denotes node indices forming an edge\n",
    "        batch: torch.Tensor\n",
    "            A 1-D tensor (n_nodes,) that tells you each node belongs to which graph\n",
    "        \"\"\"\n",
    "        node_hidden = self.node_embed(node_features) # call the node embedding\n",
    "        edge_hidden = self.edge_embed(edge_features) # call the edge embedding\n",
    "        updated_node_hidden, updated_edge_hidden = self.message_passing(\n",
    "            node_hidden,\n",
    "            edge_hidden,\n",
    "            edge_index) # call the message passing layer\n",
    "        readout = self.readout(updated_node_hidden) # use the readout layer to output \"atomic\" contributions\n",
    "        out = scatter(readout, batch) # use the scatter function to aggregate atomic readouts\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6309d75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_input_dim = 11\n",
    "edge_input_dim = 1\n",
    "node_hidden_dim = 64\n",
    "edge_hidden_dim = 64\n",
    "\n",
    "net = GraphNet(node_input_dim, edge_input_dim, node_hidden_dim, edge_hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e266cccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = next(iter(GraphDataLoader(qm9[:10], batch_size=2)))\n",
    "batch_pred = net(\n",
    "    batch_data.x, batch_data.edge_attr, \n",
    "    batch_data.edge_index, batch_data.batch\n",
    ")\n",
    "batch_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
