{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4301118a",
   "metadata": {},
   "source": [
    "# Tutorial 4 - PyTorch\n",
    "\n",
    "## Outline\n",
    "\n",
    "+ Installation & Introduction\n",
    "+ PyTorch Tensors and auto grads\n",
    "+ Building up neural network\n",
    "+ Optimizers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb5ad25",
   "metadata": {},
   "source": [
    "## Installation & Introduction\n",
    "\n",
    "+ [Official Web site](https://pytorch.org/)\n",
    "\n",
    "+ [Installation](https://pytorch.org/get-started/locally/)\n",
    "\n",
    "\n",
    "PyTorch is an open-source machine learning library widely used for deep learning applications. Developed by Facebook's AI Research lab (FAIR), it provides a flexible and intuitive framework for building and training neural networks. PyTorch is known for its ease of use, computational efficiency, and dynamic computational graph, making it a favorite among researchers and developers for both academic and industrial applications.\n",
    "\n",
    "### Key Features of PyTorch\n",
    "\n",
    "+ **Dynamic Computational Graph**: PyTorch uses a dynamic computation graph (also known as a define-by-run paradigm), meaning the graph is built on the fly as operations are performed. This makes it more intuitive and flexible, allowing for easy changes and debugging.\n",
    "\n",
    "+ **Eager Execution**: Operations in PyTorch are executed eagerly, meaning they are computed immediately without waiting for a compiled graph of operations. This allows for more interactive and dynamic development.\n",
    "\n",
    "+ **Pythonic Nature**: PyTorch is deeply integrated with Python, making it easy to use and familiar to those with Python experience. It leverages Pythonâ€™s features and libraries, allowing for seamless integration with the Python data science stack (e.g., NumPy, SciPy, Pandas).\n",
    "\n",
    "+ **Extensive Library Support**: PyTorch provides a wide range of libraries and tools for various tasks in deep learning, including computer vision (TorchVision), natural language processing (TorchText), and more. This ecosystem supports a vast array of algorithms, pre-trained models, and datasets to facilitate development and experimentation.\n",
    "\n",
    "+ **GPU Acceleration**: It supports CUDA, enabling it to leverage Nvidia GPUs for accelerated tensor computations. This makes training deep neural networks significantly faster compared to CPU-based training.\n",
    "\n",
    "+ **Community and Support**: PyTorch has a large and active community, contributing to a growing ecosystem of tools, libraries, and resources. It also enjoys robust support from major tech companies, ensuring continuous development and improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11b7859",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "\n",
    "Tensors are data structure in PyTorch to manipulate data. It is very similar to numpy.ndarray, but with support for automatic differentiation and hardware acceleration (Nvidia GPU, Apple silicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c68836fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6252c2d7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2], [3, 4]], dtype=torch.int64)\n",
    "print(type(a))\n",
    "a.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c686bdf4",
   "metadata": {},
   "source": [
    "Bridge with NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ac73a8b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.array([[1., 2.], [3., 4.]])\n",
    "arr_torch = torch.from_numpy(arr)\n",
    "arr_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "500e6701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_torch_2 = torch.tensor(arr)\n",
    "arr+= 1\n",
    "arr_torch_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a5632f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [3., 4.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# detach() stops a tensor from tracking history in automatic differentiation\n",
    "arr_np = arr_torch_2.detach().numpy()\n",
    "arr_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681ea6f3",
   "metadata": {},
   "source": [
    "Generate random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c95390c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2123, -1.5801,  0.1543],\n",
       "        [ 0.5049,  1.3548,  0.8801],\n",
       "        [-0.3158, -0.0684, -0.2944],\n",
       "        [ 0.2007, -0.5693,  0.8686]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normal distribution\n",
    "torch.randn(4,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "621b48c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0154, 0.8571, 0.0700],\n",
       "        [0.0885, 0.3157, 0.6699],\n",
       "        [0.9285, 0.2943, 0.8468],\n",
       "        [0.7247, 0.1792, 0.7754]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uniform distribution\n",
    "torch.rand(4,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc611ad0",
   "metadata": {},
   "source": [
    "Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "735fe059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# arange\n",
    "torch.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb73e7eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  0.1010,  0.2020,  0.3030,  0.4040,  0.5051,  0.6061,  0.7071,\n",
       "         0.8081,  0.9091,  1.0101,  1.1111,  1.2121,  1.3131,  1.4141,  1.5152,\n",
       "         1.6162,  1.7172,  1.8182,  1.9192,  2.0202,  2.1212,  2.2222,  2.3232,\n",
       "         2.4242,  2.5253,  2.6263,  2.7273,  2.8283,  2.9293,  3.0303,  3.1313,\n",
       "         3.2323,  3.3333,  3.4343,  3.5354,  3.6364,  3.7374,  3.8384,  3.9394,\n",
       "         4.0404,  4.1414,  4.2424,  4.3434,  4.4444,  4.5455,  4.6465,  4.7475,\n",
       "         4.8485,  4.9495,  5.0505,  5.1515,  5.2525,  5.3535,  5.4545,  5.5556,\n",
       "         5.6566,  5.7576,  5.8586,  5.9596,  6.0606,  6.1616,  6.2626,  6.3636,\n",
       "         6.4646,  6.5657,  6.6667,  6.7677,  6.8687,  6.9697,  7.0707,  7.1717,\n",
       "         7.2727,  7.3737,  7.4747,  7.5758,  7.6768,  7.7778,  7.8788,  7.9798,\n",
       "         8.0808,  8.1818,  8.2828,  8.3838,  8.4848,  8.5859,  8.6869,  8.7879,\n",
       "         8.8889,  8.9899,  9.0909,  9.1919,  9.2929,  9.3939,  9.4950,  9.5960,\n",
       "         9.6970,  9.7980,  9.8990, 10.0000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linspace\n",
    "torch.linspace(0,10,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a69784a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ones & zeros\n",
    "torch.ones(6)\n",
    "torch.zeros(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aed4815d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(4,3)\n",
    "torch.ones_like(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e00cd51",
   "metadata": {},
   "source": [
    "Attributes of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ab19db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "tensor.shape[0]\n",
    "tensor.dtype\n",
    "tensor.device\n",
    "\n",
    "# shape, dtype, device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d20bbd3",
   "metadata": {},
   "source": [
    "Single-element tensor can use `.item()` method to get a Python float object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb66fda1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([4.])\n",
    "a.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0478cf7d",
   "metadata": {},
   "source": [
    "**PyTorch** can work on different hardwares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "195a284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "# send the tensor to device\n",
    "tensor_device = tensor.to(device)\n",
    "\n",
    "# send the tensor back to cpu\n",
    "tensor_cpu = tensor_device.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007743e3",
   "metadata": {},
   "source": [
    "### Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47128446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 4.],\n",
       "        [6., 8.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2], [3, 4]], dtype=torch.float, requires_grad=True)\n",
    "y = torch.sum(x ** 2)\n",
    "# backward\n",
    "y.backward()\n",
    "# get grad\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ff51c2",
   "metadata": {},
   "source": [
    "## Build Neural Network with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71a6dda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27848c3",
   "metadata": {},
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1613157e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.4810, -4.3289],\n",
      "        [ 3.6732,  4.3279],\n",
      "        [ 3.6227, -3.4451]])\n",
      "tensor([[0.0000, 0.0000],\n",
      "        [3.6732, 4.3279],\n",
      "        [3.6227, 0.0000]])\n",
      "tensor([[-0.9997, -0.9997],\n",
      "        [ 0.9987,  0.9997],\n",
      "        [ 0.9986, -0.9980]])\n",
      "tensor([[0.0112, 0.0130],\n",
      "        [0.9752, 0.9870],\n",
      "        [0.9740, 0.0309]])\n",
      "tensor([[4.6205e-01, 5.3795e-01],\n",
      "        [3.4194e-01, 6.5806e-01],\n",
      "        [9.9915e-01, 8.5144e-04]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d9/6wjn536x21n4fgb_hwj80bx80000gp/T/ipykernel_86206/1059259351.py:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(Softmax(tensor))\n"
     ]
    }
   ],
   "source": [
    "tensor = 5 * (torch.rand(3, 2) * 2 - 1)\n",
    "print(tensor)\n",
    "\n",
    "# ReLU\n",
    "relu = nn.ReLU()\n",
    "print(relu(tensor))\n",
    "\n",
    "# Tanh\n",
    "tanh = nn.Tanh()\n",
    "print(tanh(tensor))\n",
    "\n",
    "# Sigmoid\n",
    "sigmoid = nn.Sigmoid()\n",
    "print(sigmoid(tensor))\n",
    "# Softmax\n",
    "Softmax = nn.Softmax()\n",
    "print(Softmax(tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602d5ee0",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52c298a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1130)\n",
      "tensor([[0.6123, 0.2574],\n",
      "        [0.8333, 0.9689],\n",
      "        [0.5896, 0.7312],\n",
      "        [0.9963, 0.2132],\n",
      "        [0.3175, 0.8414],\n",
      "        [0.0637, 0.7856],\n",
      "        [0.0115, 0.6907],\n",
      "        [0.7475, 0.2844],\n",
      "        [0.8100, 0.6398],\n",
      "        [0.3529, 0.0498]])\n",
      "tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 1])\n",
      "tensor(0.6223)\n"
     ]
    }
   ],
   "source": [
    "# mse\n",
    "mse = nn.MSELoss()\n",
    "a, b = torch.rand(5, 2), torch.rand(5, 2)\n",
    "print(mse(a, b))\n",
    "\n",
    "# cross-entropy\n",
    "cross_entropy = nn.CrossEntropyLoss() # more reasonable loss function \n",
    "a = torch.rand(10, 2)\n",
    "print(a)\n",
    "b = torch.randint(2, (10,))\n",
    "print(b)\n",
    "print(cross_entropy(a, b)) # low number  --> similar distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f0c3c7",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40b98f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=13, out_features=3, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=3, out_features=3, bias=True)\n",
       "    (3): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # create a net with one hidden layer\n",
    "        # input_dim 13, hidden_dim 3, output_dim 3\n",
    "        # use ReLU and softmax activation func\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(13,3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3,3),\n",
    "            nn.Softmax(dim =1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.layers(X)\n",
    "    \n",
    "\n",
    "model = Net()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77966fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: layers.0.weight | Size: torch.Size([3, 13]) | Values : tensor([[ 0.2420, -0.1840, -0.1041, -0.1980,  0.0645, -0.2736, -0.0659,  0.0006,\n",
      "          0.2485,  0.1369, -0.1444,  0.0494, -0.1201],\n",
      "        [-0.2297,  0.0578, -0.0134, -0.0968, -0.1640,  0.2552,  0.1632,  0.1480,\n",
      "         -0.0308,  0.0905,  0.1821, -0.0033,  0.0651],\n",
      "        [-0.1257, -0.0071,  0.1277,  0.2254, -0.0067, -0.2448, -0.1086,  0.0676,\n",
      "         -0.1130, -0.0836,  0.1829,  0.0643, -0.2081]]) \n",
      "\n",
      "Layer: layers.0.bias | Size: torch.Size([3]) | Values : tensor([0.2745, 0.2123, 0.1118]) \n",
      "\n",
      "Layer: layers.2.weight | Size: torch.Size([3, 3]) | Values : tensor([[-0.0234,  0.1145, -0.0681],\n",
      "        [ 0.5418,  0.5636, -0.3831],\n",
      "        [-0.2218,  0.0473,  0.5681]]) \n",
      "\n",
      "Layer: layers.2.bias | Size: torch.Size([3]) | Values : tensor([-0.3683,  0.0620,  0.3707]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param.data} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6bad3456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1992, 0.3479, 0.4529],\n",
      "        [0.2032, 0.3842, 0.4125],\n",
      "        [0.1930, 0.4417, 0.3653]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(3, 13)\n",
    "y = model(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97485371",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54ab88a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alcohol %</th>\n",
       "      <th>Malic Acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alkalinity</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Phenols.1</th>\n",
       "      <th>Proantho-cyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280 315</th>\n",
       "      <th>Proline</th>\n",
       "      <th>Start assignment</th>\n",
       "      <th>ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.8</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.83</td>\n",
       "      <td>1.64</td>\n",
       "      <td>2.17</td>\n",
       "      <td>14.0</td>\n",
       "      <td>97</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.98</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.98</td>\n",
       "      <td>5.20</td>\n",
       "      <td>1.08</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1045</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.12</td>\n",
       "      <td>1.48</td>\n",
       "      <td>2.32</td>\n",
       "      <td>16.8</td>\n",
       "      <td>95</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.43</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.57</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.17</td>\n",
       "      <td>2.82</td>\n",
       "      <td>1280</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.75</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.41</td>\n",
       "      <td>16.0</td>\n",
       "      <td>89</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.81</td>\n",
       "      <td>5.60</td>\n",
       "      <td>1.15</td>\n",
       "      <td>2.90</td>\n",
       "      <td>1320</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Alcohol %  Malic Acid   Ash  Alkalinity   Mg  Phenols  Flavanoids  \\\n",
       "0      14.23        1.71  2.43        15.6  127      2.8        3.06   \n",
       "1      13.24        2.59  2.87        21.0  118      2.8        2.69   \n",
       "2      14.83        1.64  2.17        14.0   97      2.8        2.98   \n",
       "3      14.12        1.48  2.32        16.8   95      2.2        2.43   \n",
       "4      13.75        1.73  2.41        16.0   89      2.6        2.76   \n",
       "\n",
       "   Phenols.1  Proantho-cyanins  Color intensity   Hue  OD280 315  Proline  \\\n",
       "0       0.28              2.29             5.64  1.04       3.92     1065   \n",
       "1       0.39              1.82             4.32  1.04       2.93      735   \n",
       "2       0.29              1.98             5.20  1.08       2.85     1045   \n",
       "3       0.26              1.57             5.00  1.17       2.82     1280   \n",
       "4       0.29              1.81             5.60  1.15       2.90     1320   \n",
       "\n",
       "   Start assignment  ranking  \n",
       "0                 1        1  \n",
       "1                 1        1  \n",
       "2                 1        1  \n",
       "3                 1        1  \n",
       "4                 1        1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"/Users/mac_1/Desktop/CHEM C142/wines.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e92f8010",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1166212558746338\n",
      "1.114992618560791\n",
      "1.1133668422698975\n",
      "1.111743450164795\n",
      "1.110122561454773\n",
      "1.1085048913955688\n",
      "1.1068904399871826\n",
      "1.1052794456481934\n",
      "1.1036324501037598\n",
      "1.1019701957702637\n",
      "1.1003565788269043\n",
      "1.0987454652786255\n",
      "1.0971381664276123\n",
      "1.095518708229065\n",
      "1.0938935279846191\n",
      "1.0922763347625732\n",
      "1.0906411409378052\n",
      "1.0890021324157715\n",
      "1.0873675346374512\n",
      "1.085737705230713\n",
      "1.0841106176376343\n",
      "1.0824875831604004\n",
      "1.0808680057525635\n",
      "1.0792546272277832\n",
      "1.0776454210281372\n",
      "1.0760397911071777\n",
      "1.0744372606277466\n",
      "1.0728365182876587\n",
      "1.07123863697052\n",
      "1.069644570350647\n",
      "1.0680569410324097\n",
      "1.0664745569229126\n",
      "1.0648969411849976\n",
      "1.063323974609375\n",
      "1.0617568492889404\n",
      "1.0601928234100342\n",
      "1.0586313009262085\n",
      "1.057071566581726\n",
      "1.0555148124694824\n",
      "1.0539599657058716\n",
      "1.0524067878723145\n",
      "1.0508428812026978\n",
      "1.0492675304412842\n",
      "1.047693133354187\n",
      "1.0461186170578003\n",
      "1.0445444583892822\n",
      "1.042970895767212\n",
      "1.0413872003555298\n",
      "1.0397734642028809\n",
      "1.0381560325622559\n",
      "1.0365365743637085\n",
      "1.0349156856536865\n",
      "1.0332932472229004\n",
      "1.0316693782806396\n",
      "1.0300602912902832\n",
      "1.0284558534622192\n",
      "1.0268492698669434\n",
      "1.0252403020858765\n",
      "1.0236284732818604\n",
      "1.02201247215271\n",
      "1.0203922986984253\n",
      "1.0187846422195435\n",
      "1.0171769857406616\n",
      "1.0155644416809082\n",
      "1.0139473676681519\n",
      "1.0123263597488403\n",
      "1.0107122659683228\n",
      "1.0091052055358887\n",
      "1.0075130462646484\n",
      "1.0059247016906738\n",
      "1.0043588876724243\n",
      "1.0027978420257568\n",
      "1.0012377500534058\n",
      "0.9996719360351562\n",
      "0.9981030225753784\n",
      "0.9965317845344543\n",
      "0.9949579238891602\n",
      "0.9933507442474365\n",
      "0.9917313456535339\n",
      "0.9900968074798584\n",
      "0.9884593486785889\n",
      "0.9868199825286865\n",
      "0.9851860404014587\n",
      "0.9835572838783264\n",
      "0.9819357395172119\n",
      "0.9803144931793213\n",
      "0.9786936640739441\n",
      "0.9770736694335938\n",
      "0.9754548668861389\n",
      "0.9738373160362244\n",
      "0.9722216725349426\n",
      "0.9705968499183655\n",
      "0.968969464302063\n",
      "0.9673500061035156\n",
      "0.9657312035560608\n",
      "0.9641129970550537\n",
      "0.9624956846237183\n",
      "0.9608803987503052\n",
      "0.9592678546905518\n",
      "0.9576581120491028\n"
     ]
    }
   ],
   "source": [
    "features = df.drop(['Start assignment', 'ranking'], axis=1).values\n",
    "X = StandardScaler().fit_transform(features)\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(df['ranking'].values - 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# define loss\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "epochs = 100\n",
    "for _ in range(epochs):\n",
    "    y_pred = model(X_train)\n",
    "    loss = loss_func(y_pred, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward() # ask torch to calculate grad of parameter\n",
    "    optimizer.step() # gradient descent with momentum(algorithm of Adam)\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_loss = loss_func(model(X_test), y_test)\n",
    "        print(test_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8efb09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "c142",
   "language": "python",
   "name": "c142"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
